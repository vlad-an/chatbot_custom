{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/vladandreichuk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Imports for data handling and manipulation\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "# PyTorch imports for building and training neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Natural Language Toolkit for preprocessing and tokenization\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ensure that NLTK resources are downloaded (e.g., punkt tokenizer)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Convokit for downloading and processing the Cornell Movie-Dialogs Corpus\n",
    "from convokit import Corpus, download\n",
    "\n",
    "# Check if CUDA is available for GPU acceleration, else use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word2index = {\"SOS\": 0, \"EOS\": 1, \"PAD\": 2}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"PAD\"}\n",
    "        self.num_words = 3  # Count SOS, EOS, and PAD\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in nltk.word_tokenize(sentence):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "def load_and_preprocess_data():\n",
    "    corpus = Corpus(filename=download(\"movie-corpus\"))\n",
    "    qa_pairs = []\n",
    "    for convo in corpus.iter_conversations():\n",
    "        for i in range(len(convo.get_utterance_ids()) - 1):\n",
    "            input_line = convo.get_utterance(convo.get_utterance_ids()[i]).text\n",
    "            target_line = convo.get_utterance(convo.get_utterance_ids()[i + 1]).text\n",
    "            input_line = normalize_string(input_line)\n",
    "            target_line = normalize_string(target_line)\n",
    "            qa_pairs.append([input_line, target_line])\n",
    "    return qa_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the vocabulary\n",
    "def build_vocab(qa_pairs):\n",
    "    vocab = Vocabulary()\n",
    "    for pair in qa_pairs:\n",
    "        vocab.add_sentence(pair[0])\n",
    "        vocab.add_sentence(pair[1])\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading movie-corpus to /Users/vladandreichuk/.convokit/downloads/movie-corpus\n",
      "Downloading movie-corpus from http://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip (40.9MB)... Done\n"
     ]
    }
   ],
   "source": [
    "qa_pairs = load_and_preprocess_data()\n",
    "vocab = build_vocab(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, and test sets\n",
    "train_val_pairs, test_pairs = train_test_split(qa_pairs, test_size=0.1)\n",
    "train_pairs, val_pairs = train_test_split(train_val_pairs, test_size=0.11)  # 0.11 * 0.9 â‰ˆ 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10  # Maximum sentence length for filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogueDataset(Dataset):\n",
    "    def __init__(self, pairs, vocab):\n",
    "        self.pairs = pairs\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def indexes_from_sentence(self, sentence):\n",
    "        return [self.vocab.word2index[word] for word in nltk.word_tokenize(sentence)] + [self.vocab.word2index[\"EOS\"]]\n",
    "\n",
    "    def pad_sequence(self, sequence, max_length):\n",
    "        padded_sequence = sequence + [self.vocab.word2index[\"PAD\"]] * (max_length - len(sequence))\n",
    "        return padded_sequence\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_sequence = self.indexes_from_sentence(self.pairs[idx][0])\n",
    "        target_sequence = self.indexes_from_sentence(self.pairs[idx][1])\n",
    "        max_length = max(len(input_sequence), len(target_sequence))\n",
    "        input_padded = self.pad_sequence(input_sequence, max_length)\n",
    "        target_padded = self.pad_sequence(target_sequence, max_length)\n",
    "        return {\n",
    "            'input_tensor': torch.tensor(input_padded, dtype=torch.long),\n",
    "            'target_tensor': torch.tensor(target_padded, dtype=torch.long),\n",
    "            'input_length': len(input_sequence),\n",
    "            'target_length': len(target_sequence)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_tensors = [item['input_tensor'] for item in batch]\n",
    "    target_tensors = [item['target_tensor'] for item in batch]\n",
    "    input_lengths = [len(input_tensor) for input_tensor in input_tensors]\n",
    "    target_lengths = [len(target_tensor) for target_tensor in target_tensors]\n",
    "\n",
    "    # Padding sequences\n",
    "    input_tensors = torch.nn.utils.rnn.pad_sequence(input_tensors, padding_value=vocab.word2index[\"PAD\"])\n",
    "    target_tensors = torch.nn.utils.rnn.pad_sequence(target_tensors, padding_value=vocab.word2index[\"PAD\"])\n",
    "\n",
    "    return {\n",
    "        'input_tensor': input_tensors,\n",
    "        'target_tensor': target_tensors,\n",
    "        'input_length': torch.tensor(input_lengths),\n",
    "        'target_length': torch.tensor(target_lengths)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers, dropout=(0 if num_layers == 1 else dropout), bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        packed = pack_padded_sequence(embedded, input_lengths, enforce_sorted=False)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, _ = pad_packed_sequence(outputs)\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        if self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "\n",
    "        attn_energies = attn_energies.t()\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding_size, hidden_size, output_size, num_layers=1, dropout=0.1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(embedding_size + hidden_size, hidden_size, num_layers, dropout=(0 if num_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        attn_weights = self.attn(last_hidden[-1], encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        context = context.transpose(0, 1)\n",
    "        rnn_output, hidden = self.gru(torch.cat((embedded, context), 2), last_hidden)\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze()\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        return output, hidden, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "hidden_size = 500\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "attn_model = 'dot'\n",
    "embedding_size = hidden_size\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "clip = 1.0\n",
    "batch_size = 64\n",
    "n_iteration = 4000  # Adjust the number of iterations as needed\n",
    "print_every = 1\n",
    "save_every = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "encoder = EncoderRNN(vocab.num_words, hidden_size, encoder_n_layers, dropout).to(device)\n",
    "decoder = DecoderRNN(attn_model, embedding_size, hidden_size, vocab.num_words, decoder_n_layers, dropout).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = DialogueDataset(train_pairs, vocab)\n",
    "val_dataset = DialogueDataset(val_pairs, vocab)\n",
    "test_dataset = DialogueDataset(test_pairs, vocab)\n",
    "\n",
    "# Data loaders for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0  # Start-of-sentence token\n",
    "EOS_token = 1  # End-of-sentence token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderRNN(\n",
       "  (embedding): Embedding(49892, 500)\n",
       "  (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (gru): GRU(1000, 500, num_layers=2, dropout=0.1)\n",
       "  (concat): Linear(in_features=1000, out_features=500, bias=True)\n",
       "  (out): Linear(in_features=500, out_features=49892, bias=True)\n",
       "  (attn): Attn()\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure dropout layers are in train mode\n",
    "encoder.train()\n",
    "decoder.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building optimizers...\n"
     ]
    }
   ],
   "source": [
    "# Initialize optimizers\n",
    "print('Building optimizers...')\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, input_length, target_tensor, target_length, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length, clip):\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    target_tensor = target_tensor.to(device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(input_tensor, input_length, None)\n",
    "\n",
    "    decoder_input = torch.tensor([[vocab.word2index['SOS']] * target_tensor.size(1)], device=device)\n",
    "    decoder_hidden = encoder_hidden[:decoder.num_layers]\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    batch_size = target_tensor.size(1)\n",
    "    print(\"Batch size: \", batch_size)\n",
    "    print(\"Target length: \", target_length)\n",
    "    max_target_len = max(target_length).item()  # Change here\n",
    "    print(\"Max target length: \", max_target_len)\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        for di in range(max_target_len):  # Adjust loop to use max_target_len\n",
    "            decoder_output, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            current_targets = target_tensor[di]\n",
    "            decoder_input = target_tensor[di].view(1, -1)  # Adjust indexing for teacher forcing\n",
    "            # Calculate loss for each item in the batch and sum\n",
    "            print(\"Decorder output and current targets shapes:\", decoder_output.shape, current_targets.shape)\n",
    "            loss_i = criterion(decoder_output, current_targets)\n",
    "            print(\"Loss: \", loss_i)\n",
    "            raise TypeError(\"Error\")\n",
    "            loss += loss_i\n",
    "    else:\n",
    "        for di in range(max_length):  # Loop up to max_length as before\n",
    "            decoder_output, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach().unsqueeze(0)  # Detach from history as input\n",
    "            # Calculate loss for each item in the batch and sum\n",
    "            loss += criterion(decoder_output, target_tensor[:, di])\n",
    "            if decoder_input.item() == vocab.word2index['EOS']:\n",
    "                break\n",
    "\n",
    "    # Adjust loss calculation to average over only the non-PAD tokens\n",
    "    loss = loss / batch_size\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / max_target_len  # Return average loss per token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(model_name, train_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, n_iteration, print_every, save_every, clip, max_length):\n",
    "    print(\"Starting Training!\")\n",
    "    print_loss_total = 0\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=vocab.word2index['PAD'])\n",
    "\n",
    "    for iteration in range(1, n_iteration + 1):\n",
    "        training_batch = next(iter(train_loader))\n",
    "        input_tensor = training_batch['input_tensor']\n",
    "        target_tensor = training_batch['target_tensor']\n",
    "        input_length = training_batch['input_length']\n",
    "        target_length = training_batch['target_length']\n",
    "        \n",
    "        loss = train(input_tensor, input_length, target_tensor, target_length, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length, clip)\n",
    "\n",
    "        print_loss_total += loss\n",
    "        # plot_loss_total += loss\n",
    "\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print(f'Iteration: {iteration}; Average Loss: {print_loss_avg:.4f}')\n",
    "\n",
    "        if iteration % save_every == 0:\n",
    "            directory = os.path.join('saved_models', model_name)\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'en': encoder.state_dict(),\n",
    "                'de': decoder.state_dict(),\n",
    "                'en_opt': encoder_optimizer.state_dict(),\n",
    "                'de_opt': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'vocab_dict': vocab.__dict__,\n",
    "            }, os.path.join(directory, f'{model_name}_{iteration}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a path to save the model\n",
    "model_name = \"seq2seq_chatbot\"\n",
    "save_dir = \"./\"  # Adjust as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training!\n",
      "Batch size:  64\n",
      "Target length:  tensor([11, 10, 29,  8, 16, 19,  8, 24, 32, 26,  8, 22,  7, 60,  5, 39, 16, 16,\n",
      "        16, 10, 14,  6, 28, 12, 21, 74, 30,  8, 35, 14,  4, 12, 19, 17, 27,  8,\n",
      "        26, 26, 38, 16, 16, 21,  6, 16, 10, 34, 10, 18, 19, 12, 48, 32, 26, 44,\n",
      "         8,  5, 29, 12, 26, 15, 35, 15, 16,  3])\n",
      "Max target length:  74\n",
      "Decorder output and current targets shapes: torch.Size([64, 49892]) torch.Size([64])\n",
      "Loss:  tensor(10.8176, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/vladandreichuk/Desktop/git_reps/chatbot_custom/chatbot.ipynb Cell 27\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vladandreichuk/Desktop/git_reps/chatbot_custom/chatbot.ipynb#Y123sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Start the training\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/vladandreichuk/Desktop/git_reps/chatbot_custom/chatbot.ipynb#Y123sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m trainIters(model_name, train_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, n_iteration, print_every, save_every, clip,max_length\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "\u001b[1;32m/Users/vladandreichuk/Desktop/git_reps/chatbot_custom/chatbot.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vladandreichuk/Desktop/git_reps/chatbot_custom/chatbot.ipynb#Y123sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m input_length \u001b[39m=\u001b[39m training_batch[\u001b[39m'\u001b[39m\u001b[39minput_length\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vladandreichuk/Desktop/git_reps/chatbot_custom/chatbot.ipynb#Y123sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m target_length \u001b[39m=\u001b[39m training_batch[\u001b[39m'\u001b[39m\u001b[39mtarget_length\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/vladandreichuk/Desktop/git_reps/chatbot_custom/chatbot.ipynb#Y123sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m loss \u001b[39m=\u001b[39m train(input_tensor, input_length, target_tensor, target_length, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length, clip)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vladandreichuk/Desktop/git_reps/chatbot_custom/chatbot.ipynb#Y123sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m print_loss_total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vladandreichuk/Desktop/git_reps/chatbot_custom/chatbot.ipynb#Y123sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# plot_loss_total += loss\u001b[39;00m\n",
      "\u001b[1;32m/Users/vladandreichuk/Desktop/git_reps/chatbot_custom/chatbot.ipynb Cell 27\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vladandreichuk/Desktop/git_reps/chatbot_custom/chatbot.ipynb#Y123sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m         loss_i \u001b[39m=\u001b[39m criterion(decoder_output, current_targets)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vladandreichuk/Desktop/git_reps/chatbot_custom/chatbot.ipynb#Y123sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoss: \u001b[39m\u001b[39m\"\u001b[39m, loss_i)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/vladandreichuk/Desktop/git_reps/chatbot_custom/chatbot.ipynb#Y123sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vladandreichuk/Desktop/git_reps/chatbot_custom/chatbot.ipynb#Y123sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m         loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_i\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vladandreichuk/Desktop/git_reps/chatbot_custom/chatbot.ipynb#Y123sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: Error"
     ]
    }
   ],
   "source": [
    "# Start the training\n",
    "trainIters(model_name, train_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, n_iteration, print_every, save_every, clip,max_length=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sentence(encoder, decoder, sentence, vocab, device, max_length=10):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(vocab, sentence, device)\n",
    "        input_length = input_tensor.size(0)\n",
    "        encoder_hidden = encoder.initHidden(device)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor, torch.tensor([input_length], device=device))\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "\n",
    "        decoded_words = []\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(index2word[topi.item()])\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bleu(encoder, decoder, pairs, vocab, device):\n",
    "    smoothing_function = SmoothingFunction().method1\n",
    "    references = []\n",
    "    candidates = []\n",
    "    for input_sentence, reference_sentence in pairs:\n",
    "        reference = word_tokenize(reference_sentence)\n",
    "        candidate = evaluate_sentence(encoder, decoder, input_sentence, vocab, device)\n",
    "        if '<EOS>' in candidate:\n",
    "            candidate.remove('<EOS>')\n",
    "        references.append([reference])\n",
    "        candidates.append(candidate)\n",
    "    score = corpus_bleu(references, candidates, smoothing_function=smoothing_function)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs(conversations):\n",
    "    \"\"\"Creates input-target pairs from conversations\"\"\"\n",
    "    pairs = []\n",
    "    for conv in conversations:\n",
    "        for i in range(len(conv) - 1):\n",
    "            input_sentence = ' '.join(conv[i])  # Join tokens into a single string\n",
    "            target_sentence = ' '.join(conv[i + 1])\n",
    "            pairs.append((input_sentence, target_sentence))\n",
    "    return pairs\n",
    "\n",
    "test_pairs = create_pairs(test_conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming test_pairs is a list of (input_sentence, reference_sentence) pairs\n",
    "bleu_score = evaluate_bleu(encoder, decoder, test_pairs, vocab, device)\n",
    "print(f'BLEU score: {bleu_score:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
