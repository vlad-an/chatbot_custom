{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/vladandreichuk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Imports for data handling and manipulation\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "# PyTorch imports for building and training neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Natural Language Toolkit for preprocessing and tokenization\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ensure that NLTK resources are downloaded (e.g., punkt tokenizer)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Convokit for downloading and processing the Cornell Movie-Dialogs Corpus\n",
    "from convokit import Corpus, download\n",
    "\n",
    "# Check if CUDA is available for GPU acceleration, else use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading movie-corpus to /Users/vladandreichuk/.convokit/downloads/movie-corpus\n",
      "Downloading movie-corpus from http://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip (40.9MB)... Done\n"
     ]
    }
   ],
   "source": [
    "# Data loading and preprocessing\n",
    "corpus = Corpus(filename=download(\"movie-corpus\"))\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def normalize_string(s):\n",
    "    # Convert to ASCII\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    # Replace dots with nothing\n",
    "    s = s.replace('.', '')\n",
    "    # Space + punctuation to ensure tokens like \"hello?\" are treated as \"hello ?\"\n",
    "    s = re.sub(r\"([!?])\", r\" \\1\", s)\n",
    "    # Remove any characters that are not a sequence of lower/upper case letters or the retained punctuation marks\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s\n",
    "# Extract sentence pairs\n",
    "all_pairs = []\n",
    "for conv in corpus.get_conversation_ids():\n",
    "    conversation = corpus.get_conversation(conv)\n",
    "    utterance_ids = conversation.get_utterance_ids()\n",
    "    for i in range(len(utterance_ids) - 1):\n",
    "        utt1 = corpus.get_utterance(utterance_ids[i])\n",
    "        utt2 = corpus.get_utterance(utterance_ids[i + 1])\n",
    "        if utt1 and utt2:  # Ensure both utterances are not None\n",
    "            all_pairs.append([normalize_string(utt2.text), normalize_string(utt1.text)])\n",
    "\n",
    "# Flatten the list and filter out too long sentences\n",
    "all_pairs = [\n",
    "    pair\n",
    "    for pair in all_pairs\n",
    "    if 2 <= len(pair[0].split(\" \")) <= 10 and 2 <= len(pair[1].split(\" \")) <= 10\n",
    "]\n",
    "all_pairs[:10000]\n",
    "# Reduce dataset size\n",
    "sample_pairs = random.sample(all_pairs, int(len(all_pairs) * 1))\n",
    "train_pairs, val_pairs = train_test_split(sample_pairs, test_size=0.2)\n",
    "len(train_pairs), len(val_pairs)\n",
    "# Default word tokens\n",
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token\n",
    "# Vocabulary class\n",
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3  # Count default tokens\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "voc = Voc('movie-corpus')\n",
    "\n",
    "for pair in train_pairs:\n",
    "    voc.add_sentence(pair[0])\n",
    "    voc.add_sentence(pair[1])\n",
    "voc.num_words\n",
    "filtered_val_pairs = []\n",
    "\n",
    "# Iterate through val_pairs\n",
    "for pair in val_pairs:\n",
    "\n",
    "    # Split each sentence into words\n",
    "    words_0 = pair[0].split()\n",
    "    words_1 = pair[1].split()\n",
    "    \n",
    "    # Check if all words in both sentences are in the vocabulary\n",
    "    if all(word in voc.word2index for word in words_0) and all(word in voc.word2index for word in words_1):\n",
    "        \n",
    "        # If all words are in the vocabulary, append the pair to the filtered list\n",
    "        filtered_val_pairs.append(pair)\n",
    "len(val_pairs), len(filtered_val_pairs)\n",
    "val_pairs = filtered_val_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexes_from_sentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "hidden_size = 500\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "decoder_learning_ratio = 5.0\n",
    "clip = 50.0\n",
    "learning_rate = 0.0001\n",
    "n_iteration = 4000\n",
    "print_every = 1\n",
    "save_every = 100\n",
    "validate_every = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model building\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]\n",
    "        return outputs, hidden\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "        attn_energies = attn_energies.t()\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "\n",
    "encoder = EncoderRNN(hidden_size, \n",
    "                     embedding, \n",
    "                     encoder_n_layers, \n",
    "                     dropout)\n",
    "\n",
    "decoder = LuongAttnDecoderRNN('dot', \n",
    "                               embedding, \n",
    "                               hidden_size, \n",
    "                               voc.num_words,\n",
    "                               decoder_n_layers, \n",
    "                               dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bot: hello ?\n",
      "Bot: i m\n",
      "Bot: i m\n"
     ]
    }
   ],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        with torch.no_grad():\n",
    "            encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "            decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "\n",
    "            decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "            all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "            all_scores = torch.zeros([0], device=device)\n",
    "\n",
    "            for _ in range(max_length):\n",
    "                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "                decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "                all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "                all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "                decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "            \n",
    "            return all_tokens, all_scores\n",
    "\n",
    "\n",
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=10):\n",
    "    indexes_batch = [indexes_from_sentence(voc, sentence)]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch]).to(device)\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1).to(device)\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "def evaluateInput(encoder, decoder, searcher, voc):\n",
    "    input_sentence = ''\n",
    "    while True:\n",
    "        try:\n",
    "            input_sentence = input('> ')\n",
    "            if input_sentence in ('q', 'quit'): break\n",
    "            input_sentence = normalize_string(input_sentence)\n",
    "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "            output_words = [x for x in output_words if not (x == 'EOS' or x == 'PAD') ] #\n",
    "            print('Bot:', ' '.join(output_words))\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")\n",
    "\n",
    "\n",
    "# If loading on same machine the model was trained on\n",
    "checkpoint = torch.load('checkpoints/1600_checkpoint.tar')\n",
    "\n",
    "encoder_sd = checkpoint['en']\n",
    "decoder_sd = checkpoint['de']\n",
    "encoder_optimizer_sd = checkpoint['en_opt']\n",
    "decoder_optimizer_sd = checkpoint['de_opt']\n",
    "embedding_sd = checkpoint['embedding']\n",
    "voc.__dict__ = checkpoint['voc_dict']\n",
    "\n",
    "\n",
    "print('Building encoder and decoder ...')\n",
    "\n",
    "\n",
    "# Initialize word embeddings\n",
    "\n",
    "\n",
    "embedding.load_state_dict(embedding_sd)\n",
    "# Initialize encoder & decoder models\n",
    "\n",
    "encoder.load_state_dict(encoder_sd)\n",
    "decoder.load_state_dict(decoder_sd)\n",
    "\n",
    "# Use appropriate device\n",
    "\n",
    "print('Models built and ready to go!')\n",
    "# Set dropout layers to eval mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# Begin chatting \n",
    "evaluateInput(encoder, decoder, searcher, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
